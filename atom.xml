<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Praveen Rathinavelu]]></title>
  <link href="http://prathinavelu.github.io/atom.xml" rel="self"/>
  <link href="http://prathinavelu.github.io/"/>
  <updated>2014-11-30T22:59:37-05:00</updated>
  <id>http://prathinavelu.github.io/</id>
  <author>
    <name><![CDATA[Praveen Rathinavelu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Clustering Congress Through Twitter]]></title>
    <link href="http://prathinavelu.github.io/blog/2014/11/03/clustering-congress-through-twitter/"/>
    <updated>2014-11-03T08:09:56-05:00</updated>
    <id>http://prathinavelu.github.io/blog/2014/11/03/clustering-congress-through-twitter</id>
    <content type="html"><![CDATA[<p>Twitter offers a really unique opportunity for politicians to have direct and unmediated communication with their constituents. In a media landscape that has its own motivations in determining what topics to focus on, twitter should, <em>in theory</em>, give politicians the chance to emphasize their own topics and determine their own tone. I became interested in looking at how we could cluster members of congress based on how they tweet.</p>

<!-- more -->


<h1>Getting the Tweets</h1>

<p>I started with CSPAN&rsquo;s <a href="https://twitter.com/cspan/lists/members-of-congress/members">list</a> of members of congress on twitter, filtering out the accounts for congressional committees. Using twitter&rsquo;s API, I took the last 3000 tweets of each member&rsquo;s account and merged the tweets into one text document per congressman, storing them using MongoDB.</p>

<h1>Clustering Methodology</h1>

<p>Once I had my corpus (the full collection of members&#8217; documents), I used a TFID-Vectorizer to transform the text into a form we could cluster. The vectorizer creates a <a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a> value for each word in the corpus (after eliminating a list of stop words including common twitter terms like &ldquo;RT&rdquo; and &ldquo;icymi&rdquo;). It then creates a vector for each member of congress, which I used a K-means++ algorithm to cluster. I tested different parameters for my vectorizer, but found that it worked best if I included terms in at least 30% of members&#8217; documents and in no more than 70%. This eliminates geographically specific terms (state and city names) as well terms that are extremely common (like &ldquo;watch&rdquo;, &ldquo;live&rdquo;, and &ldquo;event&rdquo;). Setting these parameters allowed me to find terms that distinguished clusters in a meaningful way.</p>

<p>Note: I also attempted DBSCAN and Hierarchical clustering methods, but K-means yielded clusters that grouped congress in the most useful and interesting ways, as we&rsquo;ll see below.</p>

<h1>Cluster Results</h1>

<p>In analyzing the clusters, my primary question was how much party affiliation impacts a politician&rsquo;s tweets. By clustering based on top tf-idf words, we see extremely clear boundaries along party lines:</p>

<p><img src="http://prathinavelu.github.io/images/fletcher/fletcher1.png"></p>

<p>We see that we have clusters at either extreme, that are largely homogenous in terms of political party. In the center, we have two clusters that are divided between parties. I found 4 to be my ideal number of clusters, because as I incresed my number, it still preserved the same two clusters at each extreme and simply further segemented those closer to the center (without adding notable new information about the smaller groups). I kept 4 instead of 3 clusters because as we will see, there are meaningful distinctions between the two in the center.</p>

<p>In order to understand what each cluster of congressmen were tweeting about, I looked at the centroid of each group and took the top terms (again in terms of tf-idf) at those center points:</p>

<table>
<thead>
<tr>
<th style="text-align:left;">Cluster </th>
<th style="text-align:center;"> Top Terms</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"><strong>Cluster 0 (Republican)</strong>                 </td>
<td style="text-align:center;"> obamacare bills energy budget tax rep va border video        </td>
</tr>
<tr>
<td style="text-align:left;"><strong>Cluster 1 (General)</strong>                    </td>
<td style="text-align:center;"> va rep county bipartisan town statement staff committee hall </td>
</tr>
<tr>
<td style="text-align:left;"><strong>Cluster 2 (Current Events/Reactionary)</strong> </td>
<td style="text-align:center;"> isis ebola tune border war threat talking pres ll            </td>
</tr>
<tr>
<td style="text-align:left;"><strong>Cluster 3 (Democrat)</strong>                   </td>
<td style="text-align:center;"> gop pass workers violence agree rights pay education access  </td>
</tr>
</tbody>
</table>


<p>There are a few things to note here. First, that congressmen tend to mention the opposing party more than their own (with the republican cluster referring to obamacare frequently and the democrat cluster using gop as their top term). The terms in clusters 0 and 3 fit the general policy focus of each party very well (note that &ldquo;va&rdquo; is veteran affairs and &ldquo;ll&rdquo; is from references to 9/11). In the center, members of Cluster 1 tend to tweet about general events and procedural matters, with no truly distinguishing terms (again reflected in the party distribution of the cluster). Cluster 2 is more interesting, with terms that seem to be reactions to the most extreme and polarizing current events.</p>

<h1>Analyzing the Members</h1>

<p>Another way of looking at how we divide the clusters is by the profile of its members. For each group, I looked at the 5 members closest to its center (the most representative of the average member within each group):</p>

<table>
<thead>
<tr>
<th style="text-align:left;">Cluster 0       </th>
<th>   </th>
<th>    </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">Robert Aderholt </td>
<td> R </td>
<td> AL </td>
</tr>
<tr>
<td style="text-align:left;">Mike Pompeo     </td>
<td> R </td>
<td> KS </td>
</tr>
<tr>
<td style="text-align:left;">Diane Black     </td>
<td> R </td>
<td> TN </td>
</tr>
<tr>
<td style="text-align:left;">Kenny Marchant  </td>
<td> R </td>
<td> TX </td>
</tr>
<tr>
<td style="text-align:left;">Phil Gingrey    </td>
<td> R </td>
<td> GA </td>
</tr>
</tbody>
</table>


<table>
<thead>
<tr>
<th style="text-align:left;">Cluster 1       </th>
<th>   </th>
<th>    </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">Ted Yoho        </td>
<td> R </td>
<td> FL </td>
</tr>
<tr>
<td style="text-align:left;">Gregg Harper    </td>
<td> R </td>
<td> MS </td>
</tr>
<tr>
<td style="text-align:left;">Kurt Schrader   </td>
<td> D </td>
<td> OR </td>
</tr>
<tr>
<td style="text-align:left;">Cory Gardner    </td>
<td> R </td>
<td> CO </td>
</tr>
<tr>
<td style="text-align:left;">Steve Pearce    </td>
<td> R </td>
<td> NM </td>
</tr>
</tbody>
</table>


<table>
<thead>
<tr>
<th style="text-align:left;">Cluster 2       </th>
<th>   </th>
<th>    </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">Adam Kinzinger  </td>
<td> R </td>
<td> IL </td>
</tr>
<tr>
<td style="text-align:left;">Daniel Coats    </td>
<td> R </td>
<td> IN </td>
</tr>
<tr>
<td style="text-align:left;">Ed Royce        </td>
<td> R </td>
<td> CA </td>
</tr>
<tr>
<td style="text-align:left;">Mark Meadows    </td>
<td> R </td>
<td> NC </td>
</tr>
<tr>
<td style="text-align:left;">Rand Paul       </td>
<td> R </td>
<td> KY </td>
</tr>
</tbody>
</table>


<table>
<thead>
<tr>
<th style="text-align:left;">Cluster 3       </th>
<th>   </th>
<th>    </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">Zoe Lofgren     </td>
<td> D </td>
<td> CA </td>
</tr>
<tr>
<td style="text-align:left;">Nancy Pelosi    </td>
<td> D </td>
<td> CA </td>
</tr>
<tr>
<td style="text-align:left;">Alan Lowenthal  </td>
<td> D </td>
<td> CA </td>
</tr>
<tr>
<td style="text-align:left;">Gwen Moore      </td>
<td> D </td>
<td> WI </td>
</tr>
<tr>
<td style="text-align:left;">Doris Matsui    </td>
<td> D </td>
<td> CA </td>
</tr>
</tbody>
</table>


<p>The most interesting thing to note here is that in the democrat cluster, four out of the five members closest to the center are women (as well as four out of the five being from California). And despite being mixed overall, the members closest to the center of cluster 2 are all republican. I did informally use <a href="https://www.govtrack.us/congress/members">govtrack.us</a> to look at where each of these members are situated in terms of influence within their parties. At this time, there aren&rsquo;t any clear patterns between their place within party leadership and how close to they are to the center of the cluster. I would like to look at this more rigorously in the future.</p>

<h1>Visualizing the Clusters</h1>

<p>This d3 visualization shows how the clusters are distributed geographically:</p>

<div>
<!DOCTYPE html>
<meta charset="utf-8">
<style>

.counties {
  fill: none;
}

.states {
  fill: none;
  stroke: #fff;
  stroke-linejoin: round;
}

.q0-9 { fill:rgb(247,251,255); }
.q1-9 { fill:rgb(222,235,247); }
.q2-9 { fill:rgb(198,219,239); }
.q3-9 { fill:rgb(158,202,225); }
.q4-9 { fill:rgb(107,174,214); }
.q5-9 { fill:rgb(66,146,198); }
.q6-9 { fill:rgb(33,113,181); }
.q7-9 { fill:rgb(8,81,156); }
.q8-9 { fill:rgb(8,48,107); }

</style>



<body>

<div id="buttons">
<button id="cluster0">Cluster 0</button>
<button id="cluster1">Cluster 1</button>
<button id="cluster2">Cluster 2</button>
<button id="cluster3">Cluster 3</button>
</div>


<script src="http://prathinavelu.github.io/assets/d3.js"></script>
<script src="http://prathinavelu.github.io/assets/queue.v1.min.js"></script>
<script src="http://prathinavelu.github.io/assets/topojson.v1.min.js"></script>
<script>

var width = 960,
    height = 600;

var rateById = d3.map();


var quantize = d3.scale.quantize()
    .domain([0, 6])
    .range(d3.range(9).map(function(i) { return "q" + i + "-9"; }));

var projection = d3.geo.albersUsa()
    .scale(1280)
    .translate([width / 2, height / 2]);

var path = d3.geo.path()
    .projection(projection);

var svg = d3.select(".entry-content").append("svg")
    .attr("width", width)
    .attr("height", height);

var txt = "Cluster 0"
;
queue()
 .defer(d3.json, "/assets/us.json")
 .defer(d3.csv, "/assets/cluster0states.csv", function(d) {rateById.set(d.id, d.rate_adjusted); })
 .await(ready);

svg.append("text")
    .attr("x", 425)
    .attr("y", 20)
    .attr("dy", ".35em")
    .text(txt);

d3.select("#cluster0")
    .on("click", function(d,i) {
      rateById.forEach(function(key) { rateById.remove(key); });
      txt = "Cluster 0";
      queue()
        .defer(d3.json, "/assets/us.json")
        .defer(d3.csv, "/assets/cluster0states.csv", function(d) {rateById.set(d.id, d.rate_adjusted); })
        .await(ready);
    })

d3.select("#cluster1")
    .on("click", function(d,i) {
        rateById.forEach(function(key) { rateById.remove(key); });
        txt = "Cluster 1";
        queue()
        .defer(d3.json, "/assets/us.json")
        .defer(d3.csv, "/assets/cluster1states.csv", function(d) {rateById.set(d.id, d.rate_adjusted); })
        .await(ready);
    })
    
d3.select("#cluster2")
    .on("click", function(d,i) {
        rateById.forEach(function(key) { rateById.remove(key); });
        txt = "Cluster 2"
    queue()
        .defer(d3.json, "/assets/us.json")
        .defer(d3.csv, "/assets/cluster2states.csv", function(d) {rateById.set(d.id, d.rate_adjusted); })
        .await(ready);
    })
    
d3.select("#cluster3")
    .on("click", function(d,i) {
        rateById.forEach(function(key) { rateById.remove(key); });      
        txt = "Cluster 3";
    queue()
        .defer(d3.json, "/assets/us.json")
        .defer(d3.csv, "/assets/cluster3states.csv", function(d) {rateById.set(d.id, d.rate_adjusted); })
        .await(ready);
    })
    

function ready(error, us) {
  svg.selectAll("path").remove();
  svg.selectAll("text").remove();
  svg.append("g")
      .attr("class", "states")
      .selectAll("path")
      .data(topojson.feature(us, us.objects.states).features)
      .enter().append("path")
      .attr("class", function(d) {console.log(rateById.get(d.id)); return quantize(rateById.get(d.id)); })
      .attr("d", path);

  svg.append("path")
      .datum(topojson.mesh(us, us.objects.states, function(a, b) {console.log(txt); return a !== b; }))
      .attr("class", "states")
      .attr("d", path);
  
  svg.append("text")
    .attr("x", 425)
    .attr("y", 20)
    .attr("dy", ".35em")
    .text(txt);

}

d3.select(self.frameElement).style("height", height + "px");

</script>
</div>


<p>The states are shaded by what percentage of each cluster is in that state. They are also scaled by the overall percentage of congressmen from that state (so that we don&rsquo;t have states with lots of members of congress over-represented in the maps, such as California).</p>

<p>It is tricky to visualizes the shape/distribution of the clusters themselves, since each point is represented by a vector in a n-dimensional space (in our case n is 2000, the number of term features our vectorizer uses). However, using principal component analysis, we can reduce an n-dimensional space to 2 dimensions, in order to visualize it:</p>

<p><img src="http://prathinavelu.github.io/images/fletcher/fletcher3.png"></p>

<p>We see that even after losing some information by reducing to two dimensions, the basic structure of clusters is preserved, with two homogenous party clusters at either end merging into a more mixed center. It is useful to visualize the clsters in this way to see how clearly the content of congressional tweets is function of political party. We also see that democrats are slightly more densely clustered, indicating more consistency/uniformity throughout the party.</p>

<p>This image also shows that the methodology would work as a strong party classifier, given the fairly well-defined boundary lines.</p>

<h1>Conclusion</h1>

<p>The clustering approach helped us recognize clear and interesting patterns within congress, using only the content of their tweets. I would be interested in further exploring some of the characteristics of each of these groups. I already mentioned looking at where the clusters sit on the ideological and leadership spectrums, using govtrack.us. I began using NLTK and TextBlob for some sentiment analysis, but when the sentiment is averaged over each cluster, there are no clear patterns of polarity or subjectivity. It would be interesting to try other ways of detecting bias within these groups.</p>

<p><a href="https://github.com/prathinavelu/fletcher"><em>Related GitHub repository</em></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Classifying Behavior and Health Risk]]></title>
    <link href="http://prathinavelu.github.io/blog/2014/10/20/classifying-individuals-by-heart-health/"/>
    <updated>2014-10-20T18:13:06-04:00</updated>
    <id>http://prathinavelu.github.io/blog/2014/10/20/classifying-individuals-by-heart-health</id>
    <content type="html"><![CDATA[<p>At metis, we initially used a <a href="https://archive.ics.uci.edu/ml/datasets/Heart+Disease">dataset</a>
that had clinical information on patients, to explore supervised learning models that could
classify a patient as being at risk for heart disease. Somewhat predictably&ndash;with hospital-provided
data on heart rate, blood sugar, and various ekg readings&ndash;these models did pretty well.</p>

<p>However, I was more interested in finding general behavioral and lifestyle features
that could contribute to a similar sort of classifier. I ended up looking at both sleep and
occupation in relation to heart health risk.</p>

<!-- more -->


<h1>Data</h1>

<p>I used health survey <a href="https://www.ihis.us">data</a> that was collected originally for the National
Health Interview Survey. Survey data comes with its own set of inconsistencies and biases, particularly
in building an accurate machine learning model, as we&rsquo;ll see below.</p>

<h1>Sleep</h1>

<p>I started with the initial assumption that sleep would most likely have a positive linear
relationship with most health risk factors. However, once I charted sleep in relation to various
health factors, I found that people who deviate from normal sleep on either
extreme tend to have poorer health, by various measures:</p>

<p><em>hover mouse over line to highlight</em></p>

<div>
<!DOCTYPE html>
<meta charset="utf-8">
<style>

#body {
#  font: 11px sans-serif;
#}

.axis path,
.axis line {
  fill: none;
  stroke: #000;
  shape-rendering: crispEdges;
}

.x.axis path {
  display: none;
}

.line {
  fill: none;
  stroke: steelblue;
  stroke-width: 2.5px;
}

.legend rect {
  fill:white;
  stroke:black;
  opacity:1;
}


</style>
<body>
<script src="http://prathinavelu.github.io/assets/d3.js"></script>
<script src="http://prathinavelu.github.io/assets/d3.legend.js"></script>
<script>

var margin = {top: 30, right: 150, bottom: 70, left: 75},
    width = 1000 - margin.left - margin.right,
    height = 500 - margin.top - margin.bottom;


var x = d3.scale.linear()
    .range([0, width])
    .domain([3.7,11]);

var y = d3.scale.linear()
    .range([height, 0]);

var norm = d3.scale.linear().range([0,1])

var color = d3.scale.category20();

var xAxis = d3.svg.axis()
    .scale(x)
    .orient("bottom");

var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left");


var line = d3.svg.line()
    .interpolate("basis")
    .x(function(d) { return x(d.hrs); })
    .y(function(d) { return y(d.val); });

var svg = d3.select(".entry-content").append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
    .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")")
    

d3.csv("/assets/bysleepdata.csv", function(error, data) {
  //console.log(data)
  color.domain(d3.keys(data[0]).filter(function(key) { return key !== "hrsleep"; }));

  data.forEach(function(d) {
    d.hrsleep = d.hrsleep;
  });
  
  var features = color.domain().map(function(name) {
    obj = {
      name: name,
      values: data.map(function(d) {
        return +d[name]
        })
    };
    norm.domain([d3.min(obj.values),d3.max(obj.values)])
    obj.normvals = data.map(function(d) {return {hrs: d.hrsleep, val: norm(d[name])} });
    return obj;
  });
  
    console.log("features=", features)



  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(0," + height + ")")
      .call(xAxis)
      .append("text")
      .style("font-size", "11px")
      .attr("dy", "3em")
      .attr("x", 250)
      .text("Number of Hours of Sleep Per Night");
      
      

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis)
      .append("text")
      .attr("transform", "rotate(-90)")
      .attr("y", 20)
      .attr("dy", ".75em")
      .style("text-anchor", "end")
      //.text("% of People");
      
 

  var feature = svg.selectAll(".feature")
      .data(features)
      .enter().append("g")
      .attr("class", "feature");
      //.attr("class", function(d) {return "feature " + d.name});

      
  feature.append("path")
      .attr("class", "line")
      .attr("d", function(d) { console.log('data',d);return line(d.normvals); })
      .attr("data-legend",function(d) { return d.name})
      .on("mouseover", function(d){
            d3.select( this )
            .transition()
            .style( "opacity", 100)
            .style("stroke-width","6px");
            //var getname = d.name;
            var selected = d3.selectAll(".legend").selectAll('.text');
            console.log('nme',selected);
        } )
      .on( "mouseout", function() {
          d3.select( this )
            .transition()
            .style( "opacity", .3 )
            .style("stroke-width","2.5px");
              
        }  )    
      .style("stroke", function(d) { return color(d.name); })
      .style("opacity", .3);
    
    
  svg.append("g")
      .attr("class","legend")
      .attr("transform","translate(695,10)")
      .style("font-size","10px")
      //.style("opacity", .4)
      .call(d3.legend)

      
      
      
});

</script>
</div>


<p><em>Please see appendix for the small multiples version of the above chart, which will show the appropriate axis for each line.</em>
<em>The chart shows the proportion of people who said &ldquo;yes&rdquo; to questions like, &ldquo;Have you ever spoken
to a doctor about your diet&rdquo; and the actual value for features like BMI.</em></p>

<p>We see, for example, that the proportion of people who have ever head a heart attack, or smoked 100 cigarettes
in their life, dips in the healthy sleep range. Similarly, the proportion of people who frequently
exercised peaks around the healthy sleep range.</p>

<p>I then used a logistic regression classifier to predict whether a person sleeps
between 6 and 9 hours per night. The model
had the following performance:</p>

<table>
<thead>
<tr>
<th style="text-align:center;">Healthy Sleeper </th>
<th style="text-align:center;"> Precision </th>
<th style="text-align:center;"> Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">No</td>
<td style="text-align:center;">0.51</td>
<td style="text-align:center;">0.43</td>
</tr>
<tr>
<td style="text-align:center;">Yes</td>
<td style="text-align:center;">0.48</td>
<td style="text-align:center;">0.56</td>
</tr>
</tbody>
</table>


<p>This means that 51% of the people classified as unhealthy sleepers actually are, and that we
correctly classified 43% of unhealthy sleepers.</p>

<h1>Occupation</h1>

<p>Similar to oneâ€™s sleeping behavior, occupation has notable relationship with heart health.
Specifically, individuals working in Construction and Extraction have the highest heart attack
rates and individuals in Computer and Mathematical fields have the lowest heart attack rates.</p>

<p><img src="http://prathinavelu.github.io/images/mcnulty/mcnulty2.png"></p>

<h1>Classifier Performance</h1>

<p>I tested logistic regression, SVM, and Naive Bayes classifiers using our survey features&ndash;including age, sex, sleep, BMI, and occupation&ndash;to
predict heart health risk. However, we noticed inconsistencies in the survey data that prevented us from
designing an accurate model. An example was that respondents who said that they have had a heart attack before
did not always say yes to &ldquo;have you ever had a heart condition?&rdquo; This prevented us from having a consistent
method of labeling individuals as having heart health risk, and therefore building a working model.</p>

<h1>Conclusion</h1>

<p>Our exploratory analysis showed that sleep and occupation could contribute to a heart disease classifier,
as long as it was in combination with other health features, particularly clinically-derived data.</p>

<h1>Appendix</h1>

<p><img src="http://prathinavelu.github.io/images/mcnulty/mcnulty1.png"></p>

<p><a href="https://github.com/prathinavelu/mcnulty"><em>Related GitHub repository</em></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Predicting Box Office Gross for Movie Remakes]]></title>
    <link href="http://prathinavelu.github.io/blog/2014/09/22/predicting-box-office-gross-for-movie-remakes/"/>
    <updated>2014-09-22T15:38:47-04:00</updated>
    <id>http://prathinavelu.github.io/blog/2014/09/22/predicting-box-office-gross-for-movie-remakes</id>
    <content type="html"><![CDATA[<p>There is a general position that the film industry is increasingly built around sequels and remakes because they minimize risk. By basing a movie off an earlier one, we can use prior performance to constrain our uncertainty in predicting box office performance. I decided to focus on remakes, because they do not have many of the features that make sequels a relatively safe bet (they generally do not bring back the same actors or director, and are often produced decades apart). I wanted to see if, using the original version&rsquo;s performance, one could build a model that predicts the box office gross of the remake.</p>

<!-- more -->


<h1>Getting the Data</h1>

<p>In order to get my list of remakes, I used the Beautiful Soup package to scrape Wikipedia&rsquo;s list of film remakes, as well as search IMDb for movies with the remake keyword (I used both foreign and domestic remakes, but only looked at domestic gross). Once I had my full movie list, I wrote a similar scraper that searched IMDb for basic features (including budget, gross, language, and runtime) of both the original movie and its remake. Because I wanted to have enough information about the original&rsquo;s cultural presence, I wanted to use cumulative lifetime gross, which was not always the box-office gross listed on a movie&rsquo;s frontpage.</p>

<h1>Throat-clearing</h1>

<p>I based my approach around the idea that, if the original drives the performance of the remake, it&rsquo;s either because:</p>

<ul>
<li>there is something inherent in both versions (story, genre, etc.) that has a relationship to performance</li>
<li>the original&rsquo;s cultural presence boosts the appeal/value of the remake</li>
</ul>


<p>After adjusting for inflation, I looked to see if there was an obvious shape to the relationship between the box office gross of the original (its lifetime gross, according to IMDb) and remake. Not really:</p>

<p><img src="http://prathinavelu.github.io/images/luther/luther1.png"></p>

<p>We see that a high-grossing original doesn&rsquo;t necessarily guarantee a high gross for its remake. This becomes clearer when we look at the five original-remake pairs with the highest grossing originals (again, we are using the best available estimate of inflation-adjusted lifetime gross):</p>

<p><img src="http://prathinavelu.github.io/images/luther/luther2.png"></p>

<p>Since we can&rsquo;t rely on the original gross alone, I decided to see how the time between the release of the remake and original affected the remake&rsquo;s gross:</p>

<p><img src="http://prathinavelu.github.io/images/luther/luther3.png"></p>

<p>Here we see an almost-periodic relationship, with three discernible groups, whose characteristics become clearer when we look at specific data points:</p>

<ul>
<li>The first group is the smallest: remakes that were released 50+ years after the original. These are movies like <em>King Kong</em> and the <em>Jazz Singer</em> that have an enduring enough cultural presence, regardless of whether the new audience had seen the original.</li>
<li>The second group has the majority of successful remakes, with releases between 20 and 50 years after the original. These movies seem to have the benefit of being in the cultural memory, but are old enough that they could recapture a lot of the value from first-time and unfamiliar audiences. Good examples in this group include <em>Ocean&rsquo;s Eleven</em> and <em>Planet of the Apes</em>.</li>
<li>The third group is the most interesting: remakes released less than 20 years after the original. Once I incorporated additional features into the dataset, it became clear that these were primarily foreign movies remade domestically, shortly after. We can see this detailed in the figure below.</li>
</ul>


<p><img src="http://prathinavelu.github.io/images/luther/luther4.png"></p>

<p>The most successful example is <em>3 Men and a Baby</em>, which was released a year after the french movie, <em>3 hommes et un couffin</em>.</p>

<h1>Building a Model</h1>

<p>Using the scikit-learn python package, I iterated through each combination of features and ran a linear regression model. The models used ridge regularization in order to add a penalty for too many variables, which would create overfitting. I initially looked at the r-squared for models with each combination of features for some basic intuition, however, what is considered a good r-squared varies depending on the type of data we are trying to model.</p>

<p>A more useful measure of accuracy is the standard error of the model, which will show on average, how far the prediction deviates from the actual gross. To calculate this, I also used cross-validation, which tests the model on many samples within the data (again, to avoid overfitting to a particular training set) and then averages the model&rsquo;s error across the samples.</p>

<p>I found that inflation-adjusted original gross and years between releases were the two most important features in creating an accurate model. Beyond that, the only feature that improved the model significantly, was a dummy variable indicating whether the movie was of american origin or not. For the &ldquo;years between&rdquo; feature, I actually used 7 variabels of the form yearsbetween<sup>n</sup> for n up to 7, in order to model the non-linear relationship between yearsbetween and gross (that we see in the figures above).</p>

<h1>Conclusion</h1>

<p>Through cross-validation and ridge regularization, the best regression model had a standard error of roughly $44.5 million dollars. An easy way of interpreting this is that we have 95% confidence in our model&rsquo;s prediction +/- $89 million.</p>

<p>Ultimately, the current  model would not prove particularly practical for managing box-office risk. Still, the exploratory analysis provides useful insight into the kinds of movies that end up being remakes. In the future, I hope to augment this model by incorporating more features that indicate the cultural presence of the original movie. The first place I would look is IMDb&rsquo;s connections section, which talks about how many times a movie is referenced/mentioned/parodied in popular culture.</p>

<p><a href="https://github.com/prathinavelu/luther"><em>Related GitHub repository</em></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bright Lights, (not entirely) Big Data]]></title>
    <link href="http://prathinavelu.github.io/blog/2014/09/20/bright-lights/"/>
    <updated>2014-09-20T16:12:40-04:00</updated>
    <id>http://prathinavelu.github.io/blog/2014/09/20/bright-lights</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve always been interested in how we use narrative to help us cope with and understand the complex, messy systems around us. One of the most exciting things about joining the data science bootcamp at <a href="http://www.thisismetis.com/data-science">Metis</a> has been learning how to leverage computational tools into finding narrative threads within data, that would otherwise be impossible to see.</p>

<p>I&rsquo;m hoping this blog will detail some of these data stories&mdash;from both in and out of Metis. Eventually, I might put up some short fiction too.</p>
]]></content>
  </entry>
  
</feed>
